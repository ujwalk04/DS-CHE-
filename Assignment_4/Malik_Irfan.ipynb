{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "48e2eb8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "210/210 [==============================] - 1s 1ms/step - loss: 138212.5938\n",
      "Epoch 2/100\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 12517.6729\n",
      "Epoch 3/100\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 397.9250\n",
      "Epoch 4/100\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 376.1066\n",
      "Epoch 5/100\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 366.8923\n",
      "Epoch 6/100\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 356.0422\n",
      "Epoch 7/100\n",
      "210/210 [==============================] - 0s 2ms/step - loss: 343.8484\n",
      "Epoch 8/100\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 330.3883\n",
      "Epoch 9/100\n",
      "210/210 [==============================] - 0s 2ms/step - loss: 316.0185\n",
      "Epoch 10/100\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 300.5340\n",
      "Epoch 11/100\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 284.6641\n",
      "Epoch 12/100\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 267.9686\n",
      "Epoch 13/100\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 250.7272\n",
      "Epoch 14/100\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 233.4268\n",
      "Epoch 15/100\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 216.1815\n",
      "Epoch 16/100\n",
      "210/210 [==============================] - 0s 2ms/step - loss: 198.5695\n",
      "Epoch 17/100\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 181.2942\n",
      "Epoch 18/100\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 165.1554\n",
      "Epoch 19/100\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 149.7387\n",
      "Epoch 20/100\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 134.7510\n",
      "Epoch 21/100\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 121.6056\n",
      "Epoch 22/100\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 109.2583\n",
      "Epoch 23/100\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 98.0271\n",
      "Epoch 24/100\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 88.3122\n",
      "Epoch 25/100\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 79.7106\n",
      "Epoch 26/100\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 72.0357\n",
      "Epoch 27/100\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 65.4548\n",
      "Epoch 28/100\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 59.9676\n",
      "Epoch 29/100\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 54.9036\n",
      "Epoch 30/100\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 50.6862\n",
      "Epoch 31/100\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 46.8834\n",
      "Epoch 32/100\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 43.7038\n",
      "Epoch 33/100\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 41.0636\n",
      "Epoch 34/100\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 38.8732\n",
      "Epoch 35/100\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 36.8067\n",
      "Epoch 36/100\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 35.6841\n",
      "Epoch 37/100\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 34.2266\n",
      "Epoch 38/100\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 33.0738\n",
      "Epoch 39/100\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 32.2768\n",
      "Epoch 40/100\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 31.8637\n",
      "Epoch 41/100\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 31.0787\n",
      "Epoch 42/100\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 30.3915\n",
      "Epoch 43/100\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 30.0215\n",
      "Epoch 44/100\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 29.9747\n",
      "Epoch 45/100\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 29.3951\n",
      "Epoch 46/100\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 29.3635\n",
      "Epoch 47/100\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 28.7291\n",
      "Epoch 48/100\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 28.3881\n",
      "Epoch 49/100\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 28.6235\n",
      "Epoch 50/100\n",
      "210/210 [==============================] - 0s 2ms/step - loss: 28.2159\n",
      "Epoch 51/100\n",
      "210/210 [==============================] - 0s 2ms/step - loss: 27.7533\n",
      "Epoch 52/100\n",
      "210/210 [==============================] - 0s 2ms/step - loss: 27.7928\n",
      "Epoch 53/100\n",
      "210/210 [==============================] - 0s 2ms/step - loss: 27.6578\n",
      "Epoch 54/100\n",
      "210/210 [==============================] - 0s 2ms/step - loss: 27.3684\n",
      "Epoch 55/100\n",
      "210/210 [==============================] - 0s 2ms/step - loss: 27.1506\n",
      "Epoch 56/100\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 27.0591\n",
      "Epoch 57/100\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 27.1114\n",
      "Epoch 58/100\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 26.7784\n",
      "Epoch 59/100\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 26.8386\n",
      "Epoch 60/100\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 26.4660\n",
      "Epoch 61/100\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 26.4804\n",
      "Epoch 62/100\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 26.5097\n",
      "Epoch 63/100\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 26.3174\n",
      "Epoch 64/100\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 26.3767\n",
      "Epoch 65/100\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 26.8460\n",
      "Epoch 66/100\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 26.7874\n",
      "Epoch 67/100\n",
      "210/210 [==============================] - 0s 2ms/step - loss: 26.2864\n",
      "Epoch 68/100\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 26.3026\n",
      "Epoch 69/100\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 26.6320\n",
      "Epoch 70/100\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 26.3017\n",
      "Epoch 71/100\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 26.3141\n",
      "Epoch 72/100\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 26.6117\n",
      "Epoch 73/100\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 26.1152\n",
      "Epoch 74/100\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 26.6024\n",
      "Epoch 75/100\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 26.1902\n",
      "Epoch 76/100\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 26.2186\n",
      "Epoch 77/100\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 26.1862\n",
      "Epoch 78/100\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 26.6165\n",
      "Epoch 79/100\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 26.2267\n",
      "Epoch 80/100\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 26.2855\n",
      "Epoch 81/100\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 26.0349\n",
      "Epoch 82/100\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 25.9922\n",
      "Epoch 83/100\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 26.2864\n",
      "Epoch 84/100\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 26.0819\n",
      "Epoch 85/100\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 26.2543\n",
      "Epoch 86/100\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 26.3030\n",
      "Epoch 87/100\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 26.7044\n",
      "Epoch 88/100\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 26.2813\n",
      "Epoch 89/100\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 26.4273\n",
      "Epoch 90/100\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 26.9760\n",
      "Epoch 91/100\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 26.4214\n",
      "Epoch 92/100\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 25.9758\n",
      "Epoch 93/100\n",
      "210/210 [==============================] - 0s 2ms/step - loss: 26.0048\n",
      "Epoch 94/100\n",
      "210/210 [==============================] - 0s 2ms/step - loss: 26.3097\n",
      "Epoch 95/100\n",
      "210/210 [==============================] - 0s 2ms/step - loss: 26.1688\n",
      "Epoch 96/100\n",
      "210/210 [==============================] - 0s 2ms/step - loss: 26.2096\n",
      "Epoch 97/100\n",
      "210/210 [==============================] - 0s 2ms/step - loss: 26.2965\n",
      "Epoch 98/100\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 26.8716\n",
      "Epoch 99/100\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 26.3211\n",
      "Epoch 100/100\n",
      "210/210 [==============================] - 0s 1ms/step - loss: 26.2884\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1032c1fa3a0>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# read in the dataset\n",
    "df = pd.read_csv('datan.csv')\n",
    "\n",
    "# assign X and y as the features and labels of the dataset\n",
    "X = df[['AT','V','AP','RH']]\n",
    "y = df['PE']\n",
    "\n",
    "# split the data into training, validation, and test sets\n",
    "X_train, X_val_test, y_train, y_val_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=0.5, random_state=0)\n",
    "\n",
    "# define the model\n",
    "m = Sequential()\n",
    "m.add(Dense(10, input_dim=4, activation='relu'))\n",
    "m.add(Dense(1))\n",
    "\n",
    "# compile the model\n",
    "m.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# fit the model to the training data\n",
    "m.fit(X_train, y_train, epochs=100, batch_size=32)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f8b23c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Read the data from the CSV file\n",
    "data = pd.read_csv('datan.csv')\n",
    "\n",
    "# Split the dataset into input features and output\n",
    "X = data[['AT', 'V', 'AP', 'RH']].values\n",
    "y = data['PE'].values\n",
    "X=X\n",
    "y = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72952bab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9568, 4)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d7f7596",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([463.26, 444.37, 488.56, ..., 429.57, 435.74, 453.28])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9157dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train, validation and test sets\n",
    "x_train, x_val, x_test = np.split(X, [int(0.6*len(X)), int(0.8*len(X))])\n",
    "y_train, y_val, y_test = np.split(y, [int(0.6*len(y)), int(0.8*len(y))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17388f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the architecture of the Neural Network\n",
    "input_size = 4 # number of input features\n",
    "hidden_size = 5740 # number of neurons in the hidden layer\n",
    "output_size = 1 # number of output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f85872d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5740, 4)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3499aaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the weights and biases\n",
    "w1 = np.random.randn(input_size, hidden_size)\n",
    "b1 = np.random.randn(hidden_size)\n",
    "w2 = np.random.randn(hidden_size, output_size)\n",
    "b2 = np.random.randn(output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4fe63874",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the activation function\n",
    "def sigmoid(x):\n",
    "    x = np.clip(x, -700, 700)\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "582b9244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the derivative of the activation function\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c55dffdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the forward propagation\n",
    "def forward_propagation(x, w1, b1, w2, b2):\n",
    "    z1 = np.dot(x, w1) + b1\n",
    "    a1 = sigmoid(z1)\n",
    "    z2 = np.dot(a1, w2) + b2\n",
    "    a2 = sigmoid(z2)\n",
    "    return a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "85e26446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the backward propagation\n",
    "def backward_propagation(x, y, a2, w1, b1, w2, b2):\n",
    "    dz2 = a2 - y\n",
    "    dw2 = np.dot(a2.T, dz2)\n",
    "    db2 = np.sum(dz2, axis=0)\n",
    "    da1 = np.dot(dz2, w2)\n",
    "    dz1 = da1 * sigmoid_derivative(a2)\n",
    "    dw1 = np.dot(x.T, dz1)\n",
    "    db1 = np.sum(dz1, axis=0)\n",
    "    return dw1, db1, dw2, db2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ec55b3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimization algorithm\n",
    "def optimize(x, y, w1, b1, w2, b2, learning_rate):\n",
    "    a2 = forward_propagation(x, w1, b1, w2, b2)\n",
    "    dw1, db1, dw2, db2 = backward_propagation(x, y, a2, w1, b1, w2, b2)\n",
    "    w1 = w1 - learning_rate * dw1\n",
    "    b1 = b1 - learning_rate * db1\n",
    "    w2 = w2 - learning_rate * dw2\n",
    "    b2 = b2 - learning_rate * db2\n",
    "    return w1, b1, w2, b2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4696728b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of iterations (epochs) and the learning rate\n",
    "num_epochs = 500\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33078c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 205920.2983216031\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "for i in range(num_epochs):\n",
    "    w1, b1, w2, b2 = optimize(x_train, y_train, w1, b1, w2, b2, learning_rate)\n",
    "    if i % 100 == 0:\n",
    "        print(\"Epoch:\", i, \"Loss:\", np.mean((forward_propagation(x_train, w1, b1, w2, b2) - y_train)**2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe10c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the trained model to predict on the validation and test sets\n",
    "y_val_pred = forward_propagation(x_val, w1, b1, w2, b2)\n",
    "y_test_pred = forward_propagation(x_test, w1, b1, w2, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "559da6ef",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (4,1914,10) (1914,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [319], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Evaluate the performance of the model using mean squared error\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m val_mse \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean((y_val_pred \u001b[38;5;241m-\u001b[39m y_val)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m      3\u001b[0m test_mse \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean((y_test_pred \u001b[38;5;241m-\u001b[39m y_test)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation MSE:\u001b[39m\u001b[38;5;124m\"\u001b[39m, val_mse)\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (4,1914,10) (1914,) "
     ]
    }
   ],
   "source": [
    "# Evaluate the performance of the model using mean squared error\n",
    "val_mse = np.mean((y_val_pred - y_val)**2)\n",
    "test_mse = np.mean((y_test_pred - y_test)**2)\n",
    "print(\"Validation MSE:\", val_mse)\n",
    "print(\"Test MSE:\", test_mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc5fc50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
